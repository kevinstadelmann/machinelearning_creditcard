---
title: "Machine Learning Project"
author: "Jeanette Lee, Kevin Stadelmann, Marc Weber, Patricia Wellhöfer"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    css: styles.css
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
---

# Introduction
The objective of the analysis is to determine the "credit-worthiness" of applicants using socio-demographic information (gender, education, marital status, etc.) and credit history data to predict the probability of repayment and to categorize customers as "good" or "bad".  
  
Using the concept of Vintage Analysis, we summarize the credit history to determine the "percent_good", i.e. percentage of times that an individual pay back loans within 60 days, and assign "customer_status_good" to those who repay within 60 days more than 50% of the time. Varying time ranges for credit history can be used (24, 36, 48 months, etc.), in practice, however shorter periods take into account the most recent credit behaviour of each individual. For the purposes of this analysis, we have chosen a 24 month period.  
  
Data Source: [link](https://www.kaggle.com/rikdifos/credit-card-approval-prediction)
(last update: 24-03-2020)

# Setup
```{r setup, include=TRUE, warning=FALSE, message=FALSE}
library(magrittr)
library(data.table)
library(dplyr)
library(lubridate)
library(caret)
library(knitr)
library(inspectdf)
library(ggplot2)
library(GGally)
library(gridExtra)
library(car)

opts_chunk$set(fig.align="center", echo=TRUE)
knit_hooks$set(inline=function(x) { prettyNum(x, big.mark=",") })
```

# Data Preparation
## Data Import
```{r import-data, include=TRUE, cache=TRUE}
df.ar <- fread("data/application_record.csv", stringsAsFactors = TRUE)
df.cr <- fread("data/credit_record.csv", stringsAsFactors = TRUE)
```

## View Data
### Application Record Dataframe
This dataset includes the demographic data of applicants.  
- DAYS_BIRTH and DAYS_EMPLOYED columns count backwards from "current" day (0), yesterday (-1), etc.  
- DAYS_EMPLOYED is positive, if the person is currently unemployed

**Data Summary**
```{r datatypes-df-ar, echo=FALSE}
#kable(sapply(df.ar, class), col.names="type")
str(df.ar)
```
**Sample of Dataset**
```{r view-df-ar, echo=FALSE}
head(df.ar) %>% kable()
```

### Credit Record Dataframe
This dataset includes the historical data of user behaviour.  
- MONTHS_BALANCE column counts backward from "current" month (0), previous month (-1), 2 months prior (-2), etc.  
- STATUS column: 0 = 1-29 days past due, 1 = 30-59 days past due, 2 = 60-89 days overdue, 3 = 90-119 days overdue, 4 = 120-149 days overdue, 5 = overdue or bad debts, write-offs for more than 150 days, C = paid off that month, X = no loan for the month
  
**Data Summary**
```{r datatypes-df-cr, echo=FALSE}
#kable(sapply(df.cr, class), col.names="type")
str(df.cr)
```
**Sample of Dataset**
```{r view-df-cr, echo=FALSE}
head(df.cr) %>% kable()
```

## Cleaning and Joining Datasets
### Cleaning application_record.csv
Removed duplicate IDs from application_record.csv
```{r clean-df-ar, include=TRUE, collapse=TRUE}
duplicates <- df.ar %>% group_by(ID) %>% summarise(count = n()) %>% filter(count > 1)
df.ar.mod <- df.ar %>% filter(!ID %in% duplicates$ID)
```

### Cleaning credit_record.csv
The original dataset was made available on 24-03-2020, so it is presumed "current" month is March 2020. From months_balance we create a new column indicating the month for each observation. Observations older than 2 years are removed.  
  
Each ID is assigned a "good" or "bad" customer status according to their credit history. If the customer paid off their balance in under 60 days (STATUS = 0, 1, or C) for more than 50% of their balance history, they are determined to be a "good" customer, otherwise they are determined to be a "bad" customer. Additionally, the percentage of times the customer paid off their balance in 60 days is added to the dataset as "percent_good".
```{r clean-df-cr, include=TRUE}
# Extract month and year from months_balance in df.cr
df.cr$month_balance <- format(as.Date("2020-03-24") %m+% months(df.cr$MONTHS_BALANCE), "%B")

# Remove records older than 2 years
df.cr.mod <- df.cr %>% filter(MONTHS_BALANCE > -24)

# Assign customer status
df.status <- df.cr.mod %>% group_by(ID) %>%
  summarise(cnt_status_good = length(STATUS[STATUS==0|STATUS==1|STATUS=='C']), 
            cnt_status_bad = n() - cnt_status_good, 
            percent_good = round(cnt_status_good / n() * 100)) %>%
  mutate(customer_status_good = ifelse(percent_good >= 50, 1, 0)) %>%
  select(ID, customer_status_good, percent_good, cnt_status_good, cnt_status_bad)

df.status$customer_status_good <- as.factor(df.status$customer_status_good)
```

### Join datasets
The two cleaned datasets are joined and only records where IDs exist in both are kept.

Cleaning steps undertaken include:  
- converting columns to numeric (e.g. age_years from DAYS_BIRTH)  
- changing column names to lower case  
- remove outliers/fringe cases (cnt_fam_members > 7; as a result also removes cnt_children > 5)  
- combining factor levels  
- replacing null and negative values  
```{r join-datasets, include=TRUE}
df.cc.raw <- inner_join(x = df.status, y = df.ar.mod, by = "ID") %>%
  mutate(ID = as.factor(ID),
         CNT_FAM_MEMBERS = as.integer(CNT_FAM_MEMBERS),
         age_years = as.numeric(round(DAYS_BIRTH * (-1) / 365.25), 0),       # days -> years
         work_years = as.numeric(round((DAYS_EMPLOYED * (-1) / 365.25), 0)), # days -> years
         FLAG_MOBIL = as.factor(FLAG_MOBIL),
         FLAG_WORK_PHONE = as.factor(FLAG_WORK_PHONE),
         FLAG_PHONE = as.factor(FLAG_PHONE),
         FLAG_EMAIL = as.factor(FLAG_EMAIL))

setnames(df.cc.raw, tolower(names(df.cc.raw)))   # change column names to lower case
```
```{r view-levels, eval=FALSE}
df.cc.raw %>% select(-id) %>% select_if(is.factor) %>% sapply(., FUN = table)
table(df.cc.raw$cnt_fam_members)
table(df.cc.raw$cnt_children)
```
```{r join-datasets-2, include=TRUE}
df.cc.raw <- df.cc.raw[df.cc.raw$cnt_fam_members < 7, ]

# combine 'Academic degree' with 'Higher education' and set order of levels in education type
df.cc.raw$name_education_type <- recode_factor(df.cc.raw$name_education_type, "Academic degree" = "Higher education")
df.cc.raw$name_education_type <- factor(df.cc.raw$name_education_type, levels=c("Lower secondary", "Secondary / secondary special", "Incomplete higher", "Higher education"), ordered = TRUE)

df.cc.raw$work_years[df.cc.raw$work_years == '-1000'] <- NA                  # -ve = pensioner
levels(df.cc.raw$occupation_type)[levels(df.cc.raw$occupation_type)==""] <- "NA"

# select columns (exclude months_balance, days_birth, days_employed and flag_mobil)
df.cc <- df.cc.raw %>%
  select(id, 
         code_gender,
         age_years,
         name_education_type,
         name_income_type,
         name_housing_type,
         name_family_status,
         cnt_children,
         cnt_fam_members,
         flag_own_car,
         flag_own_realty,
         work_years,
         amt_income_total,
         occupation_type,
         flag_work_phone,
         flag_phone,
         flag_email,
         customer_status_good,
         percent_good,
         cnt_status_good, 
         cnt_status_bad)
```

### Remove duplicates and NA
Some IDs appear to be the same individual (with different IDs) as all other columns are the same. We only keep one observation, removing duplicates.
```{r remove-dupes}
df.cc <- df.cc[!duplicated(df.cc[c(2:17)]),]
df.cc <- na.omit(df.cc)                                                        # remove NA-values
```
```{r remove-df, include=FALSE}
rm("df.ar", "df.ar.mod", "df.cc.raw", "df.cr", "df.cr.mod", "df.status", "duplicates")
```

### Check Final Dataframe
**Data Summary**
```{r summary-df-cc-str, echo=FALSE}
str(df.cc)
```
**Sample of Dataset**
```{r summary-df-cc-head, echo=FALSE}
head(df.cc) %>% kable()
```

# Model 1: Linear Model
The scope of this analysis is to be able to predict the percentage of times that an individual pays off their credit balance within 60 days.

## Graphical Analysis
### Continuous Variables
We start the graphical analysis by plotting the response variable (i.e. percent_good) against the predictor age (i.e. age_years) and work_years, however see no clear relationship. We do, however, see a slightly negative relationship with amt_total_income.
```{r lm-ga-age, include=FALSE, message=FALSE}
ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years)) +
  geom_point() + geom_smooth()
```

```{r lm-ga-workyears, include=FALSE, message=FALSE}
ggplot(data = subset(df.cc, work_years > 0), mapping = aes(y = percent_good, x = work_years)) +
  geom_point() + geom_smooth()
```
```{r lm-ga-income, message=FALSE}
ggplot(data = df.cc, mapping = aes(y = percent_good, x = amt_income_total)) +
  geom_point() + geom_smooth()
```
Here, we can see that there are few accounts (`r count(df.cc[df.cc$amt_income_total > 750000,])`) with a total income above 750K, it would be recommended to omit or recenter focus for regression as accuracy of the prediction would be reduced in this higher income range.

### Categorical Variables
Next we plot the categorical variables against our dependent variable. The boxplots indicate that the median for name_education_type, name_housing_type, name_family_status, occupation_type, and flag_realty are the same (100%), however their factor levels differ in spread. This suggests there may exist a relationship between these variables and the dependent variable.  
```{r lm-ga-categorical, include=TRUE}
plot.ed <- ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_education_type)) +
  geom_boxplot() + theme(axis.text.x=element_text(angle=45, hjust=1)) + 
  geom_text(aes(label = ..count..), y = 103, stat = "count", size = 3, color="darkcyan") + coord_cartesian(ylim=c(0,105))
# Note: only code for one plot is shown here due to limited space
```
```{r lm-ga-categorical-plot, echo=FALSE}
plot.housing <- ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_housing_type)) +
  geom_boxplot() + theme(axis.text.x=element_text(angle=45, hjust=1)) + 
  geom_text(aes(label = ..count..), y = 103, stat = "count", size = 3, color="darkcyan") + coord_cartesian(ylim=c(0,105))

plot.occupation <- ggplot(data = df.cc, mapping = aes(y = percent_good,x = occupation_type)) +
  geom_boxplot() + theme(axis.text.x=element_text(angle=45, hjust=1)) + 
  geom_text(aes(label = ..count..), y = 105, stat = "count", size = 3, color="darkcyan", angle = 50) + coord_cartesian(ylim=c(0,105))

plot.famstatus <- ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_family_status)) +
  geom_boxplot() + theme(axis.text.x=element_text(angle=45, hjust=1)) + 
  geom_text(aes(label = ..count..), y = 103, stat = "count", size = 3, color="darkcyan") + coord_cartesian(ylim=c(0,105))

plot.realty <- ggplot(data = df.cc, mapping = aes(y = percent_good, x = flag_own_realty)) +
  geom_boxplot() + theme(axis.text.x=element_text(angle=45, hjust=1)) + 
  geom_text(aes(label = ..count..), y = 103, stat = "count", size = 3, color="darkcyan") + coord_cartesian(ylim=c(0,105))

plot.income <- ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_income_type)) + 
  geom_boxplot() + theme(axis.text.x=element_text(angle=45, hjust=1)) + 
  geom_text(aes(label = ..count..), y = 103, stat = "count", size = 3, color="darkcyan") + coord_cartesian(ylim=c(0,105))

grid.arrange(plot.ed, plot.housing, plot.famstatus, ncol=3)
grid.arrange(plot.occupation, plot.realty, plot.income, ncol=3)
```
Inspecting the predictor name_income_type, pensioners and students seem to have lower percentage of paying off loans within 60 days than other income types and so we consider this variable when fitting the model. We do need to be cautious here, as these two factor levels have very small base sizes.  
  
The remaining categorical variables seem to show no influence on percent_good, based on inspection of the boxplots.
```{r lm-ga-factor-noinfluence, include=FALSE}
ggplot(data = df.cc, mapping = aes(y = percent_good, x = code_gender)) +
  geom_boxplot()
ggplot(data = df.cc, mapping = aes(y = percent_good, x = flag_own_car)) +
  geom_boxplot()
ggplot(data = df.cc, mapping = aes(y = percent_good, x = flag_email)) +
  geom_boxplot()
ggplot(data = df.cc, mapping = aes(y = percent_good, x = flag_work_phone)) +
  geom_boxplot()
ggplot(data = df.cc, mapping = aes(y = percent_good, x = flag_phone)) +
  geom_boxplot()
```

### Count Variables
Plotting the variables cnt_fam_members and cnt_children, we see some difference in spread. Intuitively, we know there is a relationship between the two variables.
```{r lm-ga-count}
plot.children <- ggplot(data = df.cc, mapping = aes(y = percent_good, x = cnt_children, group = as.factor(cnt_children))) + geom_boxplot() +
  geom_text(aes(label = ..count..), y = 105, stat = "count", size = 3, color="darkcyan") + coord_cartesian(ylim=c(0,105))

plot.family <- ggplot(data = df.cc, mapping = aes(y = percent_good, x = cnt_fam_members, group = as.factor(cnt_fam_members))) + geom_boxplot() + 
  geom_text(aes(label = ..count..), y = 105, stat = "count", size = 3, color="darkcyan") + coord_cartesian(ylim=c(0,105))

grid.arrange(plot.children, plot.family, nrow=2)
```

### Collinearity
With a few of the variables, there is the possibility of collinearity, specifically between age_years and work_years, and cnt_fam_members and cnt_children. We inspect this by plotting the variables against one another.
```{r colinearity-years}
plot.c.years <- ggplot(data = df.cc, mapping = aes(y = work_years, x = age_years)) + geom_point()

plot.c.family <- ggplot(data = df.cc, mapping = aes(y = cnt_fam_members, x = cnt_children)) + geom_point()

grid.arrange(plot.c.years, plot.c.family, ncol=2)
```
```{r colinearity-years-vif, eval=FALSE}
vif(lm(percent_good ~ age_years + work_years, data = df.cc))
vif(lm(percent_good ~ cnt_children + cnt_fam_members, data = df.cc))
```
Looking at the count variables, we can see the pairs are correlated. With a GVIF value of `r vif(lm(percent_good ~ cnt_children + cnt_fam_members, data = df.cc))[1]`, it is recommended that one of the variables cnt_children or cnt_fam_members should be dropped.

### Interactions
Next we investigate whether there are interactions between the variables previously identified. Despite no clear relationship between age_year and percent_good, there could still be an interaction with other variables, so we start with age_year plotted against the other possible independent variables and make the following observations:  
- different housing types seem to have different slopes, suggesting an interaction with age  
- customers with 5 or 6 family members seem to be concentrated in the 30-45 age range, while the other family sizes are more distributed  
- few data points for some occupation types; no realty agents younger than 30  
  
```{r check-interact-age, message=FALSE}
ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years, group = name_housing_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_housing_type) + 
  ggtitle("Percent_good by age and housing type")
```
```{r check-interact-age-hide, include=FALSE, message=FALSE}
## variables to consider as IVs: age_years, amt_income_total, cnt_fam_members, name_education_type, name_housing_type, occupation_type, name_family_status, name_income_type
pairs(~ age_years + amt_income_total, data = subset(df.cc, work_years > 0), pch = 20, col = "darkcyan")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years, group = as.factor(cnt_fam_members))) +
  geom_point() + geom_smooth() + facet_wrap(~as.factor(cnt_fam_members)) + ggtitle("Percent_good by age and family members")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years, group = name_education_type)) +
  geom_point() + geom_smooth() + facet_wrap(~name_education_type) + ggtitle("Percent_good by age and education")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years, group = name_housing_type)) +
  geom_point() + geom_smooth() + facet_wrap(~name_housing_type) + ggtitle("Percent_good by age and housing type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years, group = occupation_type)) +
  geom_point() + geom_smooth() + facet_wrap(~occupation_type) + ggtitle("Percent_good by age and occupation")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years, group = name_family_status)) +
  geom_point() + geom_smooth() + facet_wrap(~name_family_status) + ggtitle("Percent_good by age and family status")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years, group = name_income_type)) +
  geom_point() + geom_smooth() + facet_wrap(~name_income_type) + ggtitle("Percent_good by age and income type")


ggplot(data = df.cc, mapping = aes(y = percent_good, x = age_years, group = flag_own_realty)) +
  geom_point() + geom_smooth() + facet_wrap(~flag_own_realty) + ggtitle("Percent_good by age and realty owner")
```

With amt_income_total, we identify some potential interactions with name_education_type, name_housing_type, and occupation_type.
```{r check-interact-income, message=FALSE}
ggplot(data = df.cc, mapping = aes(y = percent_good, x = amt_income_total, group = name_education_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_education_type) + 
  ggtitle("Percent_good by total income and education")
# Note: only code and single plot are shown here due to limited space
```
```{r check-interact-income-hide, include=FALSE, message=FALSE}
ggplot(data = df.cc, mapping = aes(y = percent_good, x = amt_income_total, group = name_housing_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_housing_type) + ggtitle("Percent_good by income and housing type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = amt_income_total, group = occupation_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~occupation_type) + ggtitle("Percent_good by income and occupation")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = amt_income_total, group = as.factor(cnt_fam_members))) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~as.factor(cnt_fam_members)) + ggtitle("Percent_good by income and family members")
# difference in slope of regression line by cnt_fam_members suggest interaction

ggplot(data = df.cc, mapping = aes(y = percent_good, x = amt_income_total, group = name_family_status)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_family_status) + ggtitle("Percent_good by income and family status")
# differing regression line by marital status suggests interaction

ggplot(data = df.cc, mapping = aes(y = percent_good, x = amt_income_total, group = name_income_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_income_type) + ggtitle("Percent_good by income and income type")
# no clear interaction

ggplot(data = df.cc, mapping = aes(y = percent_good, x = amt_income_total, group = flag_own_realty)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~flag_own_realty) + ggtitle("Percent_good by income and realty owner")
# lower income for non-realty owners
```
Here it seems that customers with only lower secondary education are concentrated at a lower total income (< 250K). With occupation_type (plot not displayed), we can see that some occupation types (cleaning staff, cooking staff, low-skill laborers, realty agents, secretaries, waiters) are primarily concentrated at a lower income level.  
  
We also see some interaction between cnt_fam_members and flag_own_realty.
```{r view-interact-cntfam-realty, message=FALSE}
ggplot(data = df.cc, mapping = aes(y = percent_good, x = cnt_fam_members, group = flag_own_realty)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~flag_own_realty) + 
  ggtitle("Percent_good by family members and realty owner")
```

```{r no-interactions, include=FALSE, message=FALSE}
ggplot(data = df.cc, mapping = aes(y = percent_good, x = cnt_fam_members, group = name_education_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_education_type) + ggtitle("Percent_good by family members and education")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = cnt_fam_members, group = name_housing_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_housing_type) + ggtitle("Percent_good by family members and housing type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = cnt_fam_members, group = occupation_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~occupation_type) + ggtitle("Percent_good by family members and occupation")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = cnt_fam_members, group = name_family_status)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_family_status) + ggtitle("Percent_good by family members and family status")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = cnt_fam_members, group = name_income_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_income_type) + ggtitle("Percent_good by family members and income type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_education_type, group = name_housing_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_housing_type) + ggtitle("Percent_good by education and housing type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_education_type, group = occupation_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~occupation_type) + ggtitle("Percent_good by education and occupation")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_education_type, group = name_family_status)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_family_status) + ggtitle("Percent_good by education and family status")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_education_type, group = name_income_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_income_type) + ggtitle("Percent_good by education and income type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_education_type, group = flag_own_realty)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~flag_own_realty) + ggtitle("Percent_good by education and realty owner")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_housing_type, group = occupation_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~occupation_type) + ggtitle("Percent_good by housing type and occupation")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_housing_type, group = name_family_status)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_family_status) + ggtitle("Percent_good by housing type and family status")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_housing_type, group = name_income_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_income_type) + ggtitle("Percent_good by housing type and income type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_housing_type, group = flag_own_realty)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~flag_own_realty) + ggtitle("Percent_good by housing type and realty owner")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = occupation_type, group = name_family_status)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_family_status) + ggtitle("Percent_good by occupation and family status")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = occupation_type, group = name_income_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_income_type) + ggtitle("Percent_good by occupation and income type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = occupation_type, group = flag_own_realty)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~flag_own_realty) + ggtitle("Percent_good by occupation and realty owner")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_family_status, group = name_income_type)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~name_income_type) + ggtitle("Percent_good by family status and income type")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_family_status, group = flag_own_realty)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~flag_own_realty) + ggtitle("Percent_good by family status and realty owner")

ggplot(data = df.cc, mapping = aes(y = percent_good, x = name_income_type, group = flag_own_realty)) +
  geom_point() + geom_smooth(method="lm") + facet_wrap(~flag_own_realty) + ggtitle("Percent_good by income type and realty owner")
```

Possible interactions identified from plots:  
- age_year:name_housing_type, age_year:flag_own_realty, age_year:cnt_fam_members  
- amt_income_total:name_housing_type, amt_income_total:occupation_type, amt_income_total:name_family_status, amt_income_total:name_education_type  
- cnt_fam_members:name_family_status, cnt_fam_members:education(lower secondary), cnt_fam_members:flag_own_realty  
- name_education_type:occupation_type 

## Fit Models
### Global F-test
Beginning with a quick global F-test, we check if any of the predictors have an influence on the response variable, by comparing the full model, lm.fit.1 (all variables and interactions), against a model with only the intercept (lm.fit.0). This gives evidence that at least one of predictors plays a relevant role, albeit weakly. Using drop1() suggests keeping the interactions flag_own_realty:cnt_fam_members and name_education_type:occupation_type.  
```{r lm-global, include=FALSE}
lm.fit.0 <- lm(percent_good ~ 1, data = df.cc)

lm.fit.1 <- lm(percent_good ~ amt_income_total + name_education_type + name_housing_type + name_family_status + occupation_type + flag_own_realty + name_income_type + cnt_fam_members + age_years + age_years:name_housing_type + age_years:cnt_fam_members + amt_income_total:name_education_type + amt_income_total:name_housing_type + amt_income_total:occupation_type + amt_income_total:cnt_fam_members + amt_income_total:name_family_status + cnt_fam_members:flag_own_realty + cnt_fam_members:name_education_type + cnt_fam_members:name_family_status + name_education_type:occupation_type, data = df.cc)
```
```{r lm-global-test, include=FALSE}
anova(lm.fit.0, lm.fit.1)
drop1(lm.fit.1, test = "F")
```
Refitting and checking the new model (lm.fit.2) against the model with only the intercept (lm.fit.0) gives a stronger indicator that at least one of the variables has an influence. 
```{r lm-refit, echo=FALSE}
lm.fit.2 <- lm(percent_good ~ name_education_type + occupation_type + flag_own_realty + cnt_fam_members + cnt_fam_members:flag_own_realty + name_education_type:occupation_type, data = df.cc)
```
```{r lm-refit-test}
anova(lm.fit.0, lm.fit.2)
drop1(lm.fit.2, test = "F")
```

### Simple Regression Models
An alternate method of model development, we start by fitting simple regression models with each of the identified independent variables and use summary() to test continuous variables and drop1() to test categorical variables.
```{r lm-no-interaction-hide, include=FALSE}
lm.fit.age <- lm(percent_good ~ age_years, data = df.cc)
summary(lm.fit.age)   # not statistically significant

lm.fit.income <- lm(percent_good ~ amt_income_total, data = df.cc)
summary(lm.fit.income)   # not statistically significant

lm.fit.cntfam <- lm(percent_good ~ cnt_fam_members, data = df.cc)
drop1(lm.fit.cntfam, test = "F")   # not statistically significant

lm.fit.education <- lm(percent_good ~ name_education_type, data = df.cc)
drop1(lm.fit.education, test = "F")   # not statistically significant

lm.fit.housing <- lm(percent_good ~ name_housing_type, data = df.cc)
drop1(lm.fit.housing, test = "F")   # not statistically significant
```

```{r lm-fit-occupation}
lm.fit.occupation <- lm(percent_good ~ occupation_type, data = df.cc)
drop1(lm.fit.occupation, test = "F")   # statistically significant
```
The output shows that occupation_type is statistically significant and using anova() to compare against a model that contains only an intercept, there is strong evidence the model with more parameters better fits the data (lower RSS for model with occupation_type, significant p-value), as seen in the output below.
```{r lm-fit-occ-anova}
anova(lm.fit.0, lm.fit.occupation)
```
Adding the other variables, we see that only flag_own_realty is statistically significant.
```{r lm-no-interaction2-hide, include=FALSE}
lm.fit.occ.fam <- update(lm.fit.occupation, . ~ . + name_family_status)
drop1(lm.fit.occ.fam, test = "F")   # not statistically significant

lm.fit.occ.incometype <- update(lm.fit.occupation, . ~ . + name_income_type)
drop1(lm.fit.occ.incometype, test = "F")   # not statistically significant
```

```{r lm-fit-occ-realty}
lm.fit.occ.realty <- update(lm.fit.occupation, . ~ . + flag_own_realty)
drop1(lm.fit.occ.realty, test = "F")
anova(lm.fit.occupation, lm.fit.occ.realty)
```
Comparing with the previous model (lm.fit.occupation), we see evidence that the more complex model is a better fit to the data. 

### Regression Models with Interaction
We had identified 2 variables that showed potential interaction with occupation_type, and 2 with flag_own_realty, we look at these now to determine whether interactions should be included in the model.
```{r lm-fit-interaction-hide, include=FALSE}
lm.fit.interact.1 <- update(lm.fit.occ.realty, . ~ . + amt_income_total + occupation_type:amt_income_total)
drop1(lm.fit.interact.1, test = "F")   # insignificant interaction

lm.fit.interact.2 <- update(lm.fit.occ.realty, . ~ . + name_education_type + occupation_type:name_education_type)
drop1(lm.fit.interact.2, test = "F")   # insignificant interaction

lm.fit.interact.3 <- update(lm.fit.occ.realty, . ~ . + age_years + flag_own_realty:age_years)
drop1(lm.fit.interact.3, test = "F")   # insignificant interaction
```

```{r lm-fit-interaction}
lm.fit.interact.4 <- update(lm.fit.occ.realty, . ~ . + cnt_fam_members + flag_own_realty:cnt_fam_members)
drop1(lm.fit.interact.4, test = "F")
anova(lm.fit.occ.realty, lm.fit.interact.4)
```
Here there is only weak evidence that the flag_own_realty:cnt_fam_members interaction plays a role.

## Measures of Fit 
### In-sample performance
To quantify the goodness of fit, we look at the R^2 and adjusted R^2 values:  

```{r lm-r2-insample, echo=FALSE}
r2.in <- c(round(summary(lm.fit.occupation)$r.squared, 5), round(summary(lm.fit.occ.realty)$r.squared, 5), round(summary(lm.fit.interact.4)$r.squared, 5))

adj.r2.in <- c(round(summary(lm.fit.occupation)$adj.r.squared, 5), round(summary(lm.fit.occ.realty)$adj.r.squared, 5), round(summary(lm.fit.interact.4)$adj.r.squared, 5))

r2.table <- data.frame("R-squared"= r2.in, "Adjusted R-squared" = adj.r2.in, row.names = c("occupation_type", "occupation_type + flag_own_realty", "occupation_type + flag_own_realty + flag_own_realty:cnt_fam_members"))

r2.table %>% kable()
```

We see that the model with interaction has the best R^2 value, and, taking into account the differing complexity of the models, it still shows a slightly higher adjusted R^2, but not by much.

### Out-of-sample performance
Now, we use part of the data to train the model and then test predictive performance on the remaining portion.
```{r lm-subset}
set.seed(1)
indices <- createDataPartition(df.cc$percent_good, p=.85, list=F)
train <- df.cc %>% slice(indices)
test_in <- df.cc %>% slice(-indices) %>% select(-customer_status_good, -percent_good)
test_true <- df.cc %>% slice(-indices) %>% select(percent_good)
```

```{r lm-predict}
# lm.fit.occupation
lm.train.1 <- lm(formula=formula(lm.fit.occupation), data = train)
predict.1.test <- predict(object = lm.train.1, newdata = test_in)

# lm.fit.occ.realty
lm.train.2 <- lm(formula=formula(lm.fit.occ.realty), data = train)
predict.2.test <- predict(object = lm.train.2, newdata = test_in)

# lm.fit.interact.4
lm.train.3 <- lm(formula=formula(lm.fit.interact.4), data = train)
predict.3.test <- predict(object = lm.train.3, newdata = test_in)
```
This yields the following R^2 values:
```{r lm-r2-table, echo=FALSE}
r2 <- c(round(cor(predict.1.test, test_true)^2, 5), round(cor(predict.2.test, test_true)^2, 5), round(cor(predict.3.test, test_true)^2, 5))
names(r2) <- c("occupation_type", "occupation_type + flag_own_realty", "occupation_type + flag_own_realty + flag_own_realty:cnt_fam_members")
r2 %>% kable(col.names = "R-squared")
```
Comparing the in-sample R^2 values with the outer-sample R^2 values, we see that they are consistently lower for the test data. As the model with interaction has an out-of-sample R-squared of about 3%, we conclude that it is better suited to make predictions.

# GLM (Poisson)

# GLM (Binomial)

# GAM

# SVM

# Neural Networks

# Optimization
The distinction between good and bad customer is a way to build up a relatively safe portfolio of customers, who repay their debts with the interest. However, the defined threshold leaves room for optimization.  
  
Potential options to increase earnings through optimization include:  
  
<b>1. Granting credit cards to a percentage of "bad" applicants</b> <br>
This adds a marginally higher risk to the portfolio of credit card customers, however, comes with higher possible returns.  
  
<b> 2. Allocating credit limits  </b> <br>
Determining the credit limit to allow an applicant/customer based on their likelihood to repay (i.e. "percent_good") in order to maximize interest paid, thereby increasing earnings.  
  
<b> 3. Dunning</b> <br>
It is lucrative for credit card companies when customers do not immediately repay the loan amounts they borrow. Depending on the agreed contract, the companies can claim additional fees and penalty interest from a certain point. The dunning system should take this into account and not dun the customer too early or the full amount as overdue. However, it should be noted that the later the reminder is issued, the greater the likelihood of a credit default (untested assumption). It is therefore appropriate to demand regular partial payments.

To optimize (maximize) the profit of a credit card company the following <b>simplified</b> formular could be applied:

<b>Maximize $g(x,y) =$ “loan amount” $\cdot$ “interest rate” $\cdot \frac{x}{360} +$ “additional fees” $–$ “loan amount” $\cdot$ “probability of default”$(x,y)$</b>

Control variables: <br>
$x =$ days until dunning letter will be sent<br>
$y =$ amount of partial payment

Probability of default is another function based on $x$ and $y$. The higher the value of $x$ and the lower the value of $y$, it can be assumed that the probability of default will be increased.

Constraints are defined based on the agreed contracts between the company and the customers. For example, there it can be defined the minimum days until an amount is overdue (e.g. $x >=$ 30 days).

# Session Information
```{r session-info}
sessionInfo()
```

